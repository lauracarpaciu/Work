{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "c:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\_distutils_hack\\__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "%pip install pandas\n",
    "%pip install sklearn\n",
    "import pandas as pd\n",
    "from detection import isbn_utils\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base directory: ./S1\n",
      "\n",
      "data: ./S1/data\n",
      "model: ./S1/model\n",
      "vocab: ./S1/vocab\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = './S1'\n",
    "\n",
    "# Get the subdirectories that contain the experiment files\n",
    "data, model, vocab = isbn_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "print(\n",
    "    f'base directory: {BASE_DIR}\\n\\n'\n",
    "    f'data: {data}\\n'\n",
    "    f'model: {model}\\n'\n",
    "    f'vocab: {vocab}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>domain</th>\n",
       "      <th>published_date</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW remains on COVID-19 high alert | The Star | Newcastle, NSW</td>\n",
       "      <td>https://www.newcastlestar.com.au/story/6878465/nsw-remains-on-covid-19-high-alert/?cs=7</td>\n",
       "      <td>newcastlestar.com.au</td>\n",
       "      <td>2020-08-13 17:32:28</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Missing people: How does someone just disappear?</td>\n",
       "      <td>https://www.bbc.co.uk/news/uk-england-cambridgeshire-53648599</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>2020-08-04 15:56:20</td>\n",
       "      <td>NATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An American Pickle review: In a dual role, Seth Rogen’s drama wins over comedy</td>\n",
       "      <td>https://www.polygon.com/2020/8/6/21357120/american-pickle-review-movie-seth-rogen-simon-rich-hbo-max</td>\n",
       "      <td>polygon.com</td>\n",
       "      <td>2020-08-06 15:58:22</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Katie Spotz to Run 130 Miles to Bring Clean Water to Tanzania, Shares Testimony of Change</td>\n",
       "      <td>http://www.christianitydaily.com/articles/9647/20200807/katie-spotz-run-130-miles-bring-clean-water-tanzania-shares.htm</td>\n",
       "      <td>christianitydaily.com</td>\n",
       "      <td>2020-08-07 19:45:00</td>\n",
       "      <td>NATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CDC Director Says This Fall Could Be ‘the Worst’ We’ve Ever Had Thanks to COVID-19</td>\n",
       "      <td>https://www.self.com/story/worst-fall-ever-covid</td>\n",
       "      <td>self.com</td>\n",
       "      <td>2020-08-13 19:37:52</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Free water and electricity may be counterproductive – Economist</td>\n",
       "      <td>https://www.ghanaweb.com/GhanaHomePage/NewsArchive/Free-water-and-electricity-may-be-counterproductive-Economist-1031098</td>\n",
       "      <td>ghanaweb.com</td>\n",
       "      <td>2020-08-11 13:25:37</td>\n",
       "      <td>NATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>State places Tazewell County on COVID-19 warning list</td>\n",
       "      <td>https://week.com/2020/08/07/state-places-tazewell-county-on-covid-19-warning-list/</td>\n",
       "      <td>week.com</td>\n",
       "      <td>2020-08-07 18:00:24</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Over 100 people quarantined in Mississippi school district after several test positive for coronavirus</td>\n",
       "      <td>https://www.nbcnews.com/news/us-news/over-100-people-quarantined-mississippi-school-district-after-several-test-n1236012</td>\n",
       "      <td>nbcnews.com</td>\n",
       "      <td>2020-08-06 16:35:00</td>\n",
       "      <td>NATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How global warming is wiping out Africa's oldest baobab trees</td>\n",
       "      <td>http://www.capetalk.co.za/articles/307750/how-global-warming-is-wiping-out-africa-s-oldest-baobab-trees</td>\n",
       "      <td>capetalk.co.za</td>\n",
       "      <td>2018-06-14 12:00:53</td>\n",
       "      <td>WORLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Delay routine dental checkups, WHO urges, until COVID risk is known</td>\n",
       "      <td>https://www.thepeninsulaqatar.com/article/11/08/2020/Delay-routine-dental-checkups,-WHO-urges,-until-COVID-risk-is-known</td>\n",
       "      <td>thepeninsulaqatar.com</td>\n",
       "      <td>2020-08-11 17:13:00</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    title  \\\n",
       "0                                          NSW remains on COVID-19 high alert | The Star | Newcastle, NSW   \n",
       "1                                                        Missing people: How does someone just disappear?   \n",
       "2                          An American Pickle review: In a dual role, Seth Rogen’s drama wins over comedy   \n",
       "3               Katie Spotz to Run 130 Miles to Bring Clean Water to Tanzania, Shares Testimony of Change   \n",
       "4                      CDC Director Says This Fall Could Be ‘the Worst’ We’ve Ever Had Thanks to COVID-19   \n",
       "5                                         Free water and electricity may be counterproductive – Economist   \n",
       "6                                                   State places Tazewell County on COVID-19 warning list   \n",
       "7  Over 100 people quarantined in Mississippi school district after several test positive for coronavirus   \n",
       "8                                           How global warming is wiping out Africa's oldest baobab trees   \n",
       "9                                     Delay routine dental checkups, WHO urges, until COVID risk is known   \n",
       "\n",
       "                                                                                                                       link  \\\n",
       "0                                   https://www.newcastlestar.com.au/story/6878465/nsw-remains-on-covid-19-high-alert/?cs=7   \n",
       "1                                                             https://www.bbc.co.uk/news/uk-england-cambridgeshire-53648599   \n",
       "2                      https://www.polygon.com/2020/8/6/21357120/american-pickle-review-movie-seth-rogen-simon-rich-hbo-max   \n",
       "3   http://www.christianitydaily.com/articles/9647/20200807/katie-spotz-run-130-miles-bring-clean-water-tanzania-shares.htm   \n",
       "4                                                                          https://www.self.com/story/worst-fall-ever-covid   \n",
       "5  https://www.ghanaweb.com/GhanaHomePage/NewsArchive/Free-water-and-electricity-may-be-counterproductive-Economist-1031098   \n",
       "6                                        https://week.com/2020/08/07/state-places-tazewell-county-on-covid-19-warning-list/   \n",
       "7  https://www.nbcnews.com/news/us-news/over-100-people-quarantined-mississippi-school-district-after-several-test-n1236012   \n",
       "8                   http://www.capetalk.co.za/articles/307750/how-global-warming-is-wiping-out-africa-s-oldest-baobab-trees   \n",
       "9  https://www.thepeninsulaqatar.com/article/11/08/2020/Delay-routine-dental-checkups,-WHO-urges,-until-COVID-risk-is-known   \n",
       "\n",
       "                  domain       published_date          topic  \n",
       "0   newcastlestar.com.au  2020-08-13 17:32:28         HEALTH  \n",
       "1              bbc.co.uk  2020-08-04 15:56:20         NATION  \n",
       "2            polygon.com  2020-08-06 15:58:22  ENTERTAINMENT  \n",
       "3  christianitydaily.com  2020-08-07 19:45:00         NATION  \n",
       "4               self.com  2020-08-13 19:37:52         HEALTH  \n",
       "5           ghanaweb.com  2020-08-11 13:25:37         NATION  \n",
       "6               week.com  2020-08-07 18:00:24         HEALTH  \n",
       "7            nbcnews.com  2020-08-06 16:35:00         NATION  \n",
       "8         capetalk.co.za  2018-06-14 12:00:53          WORLD  \n",
       "9  thepeninsulaqatar.com  2020-08-11 17:13:00         HEALTH  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Load the datasets into dataframes\n",
    "train_df = pd.read_csv(f'{data}/train_data.csv')\n",
    "test_df = pd.read_csv(f'{data}/test_data.csv')\n",
    "\n",
    "# Preview the first 10 rows of the training set\n",
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>28% Fall in Mumbai's Home Isolation Cases, 62% Drop in Institutional Quarantine as Covid Curve Plateaus</td>\n",
       "      <td>NATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sniffer dogs begin trials to detect Covid</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Seth Rogen says Marvel blockbusters make it difficult for comedy films</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Farewell to Pete Way, the debauched bassist with a frontman's swagger</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Poo detectives on the COVID-19 frontline</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Fortnite maker challenges Apple and Google's app store rules through direct-payment discounts</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Forex auction bolsters productive sector</td>\n",
       "      <td>NATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Intel Tiger Lake could outrun AMD Ryzen 4000 in single-core performance</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Rappi delivery workers in Colombia's Bogota strike over conditions</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      title  \\\n",
       "30  28% Fall in Mumbai's Home Isolation Cases, 62% Drop in Institutional Quarantine as Covid Curve Plateaus   \n",
       "31                                                                Sniffer dogs begin trials to detect Covid   \n",
       "32                                   Seth Rogen says Marvel blockbusters make it difficult for comedy films   \n",
       "33                                    Farewell to Pete Way, the debauched bassist with a frontman's swagger   \n",
       "34                                                                 Poo detectives on the COVID-19 frontline   \n",
       "35            Fortnite maker challenges Apple and Google's app store rules through direct-payment discounts   \n",
       "36                                 Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "37                                                                 Forex auction bolsters productive sector   \n",
       "38                                  Intel Tiger Lake could outrun AMD Ryzen 4000 in single-core performance   \n",
       "39                                       Rappi delivery workers in Colombia's Bogota strike over conditions   \n",
       "\n",
       "            topic  \n",
       "30         NATION  \n",
       "31         HEALTH  \n",
       "32  ENTERTAINMENT  \n",
       "33  ENTERTAINMENT  \n",
       "34         HEALTH  \n",
       "35     TECHNOLOGY  \n",
       "36       BUSINESS  \n",
       "37         NATION  \n",
       "38     TECHNOLOGY  \n",
       "39     TECHNOLOGY  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indices of the dataframe to use\n",
    "start_index = 30\n",
    "end_index = 40\n",
    "\n",
    "# Sample titles to label\n",
    "train_df[['title']][start_index:end_index]\n",
    "\n",
    "# # When you're done, uncomment the next line to see the 'true' labels \n",
    "train_df[['title', 'topic']][start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 24)            240000    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20, 24)            600       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 480)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 3848      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244448 (954.88 KB)\n",
      "Trainable params: 244448 (954.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = tf.keras.models.load_model(model)\n",
    "\n",
    "# Show the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': {'module': 'keras.optimizers',\n",
       "  'class_name': 'Adam',\n",
       "  'config': {'name': 'Adam',\n",
       "   'weight_decay': None,\n",
       "   'clipnorm': None,\n",
       "   'global_clipnorm': None,\n",
       "   'clipvalue': None,\n",
       "   'use_ema': False,\n",
       "   'ema_momentum': 0.99,\n",
       "   'ema_overwrite_frequency': None,\n",
       "   'jit_compile': False,\n",
       "   'is_legacy_optimizer': False,\n",
       "   'learning_rate': 0.0010000000474974513,\n",
       "   'beta_1': 0.9,\n",
       "   'beta_2': 0.999,\n",
       "   'epsilon': 1e-07,\n",
       "   'amsgrad': False},\n",
       "  'registered_name': None},\n",
       " 'loss': {'module': 'builtins',\n",
       "  'class_name': 'function',\n",
       "  'config': 'sparse_categorical_crossentropy',\n",
       "  'registered_name': 'function'},\n",
       " 'metrics': [[{'module': 'keras.metrics',\n",
       "    'class_name': 'MeanMetricWrapper',\n",
       "    'config': {'name': 'sparse_categorical_accuracy',\n",
       "     'dtype': 'float32',\n",
       "     'fn': {'module': 'builtins',\n",
       "      'class_name': 'function',\n",
       "      'config': 'sparse_categorical_accuracy',\n",
       "      'registered_name': 'function'}},\n",
       "    'registered_name': None}]],\n",
       " 'loss_weights': None,\n",
       " 'weighted_metrics': None,\n",
       " 'run_eagerly': None,\n",
       " 'steps_per_execution': None,\n",
       " 'jit_compile': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get info about compiling the model\n",
    "model.get_compile_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENTERTAINMENT',\n",
       " 'HEALTH',\n",
       " 'TECHNOLOGY',\n",
       " 'WORLD',\n",
       " 'BUSINESS',\n",
       " 'SPORTS',\n",
       " 'NATION',\n",
       " 'SCIENCE']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a lookup list for the labels\n",
    "topic_lookup = tf.keras.layers.StringLookup(vocabulary=f'{vocab}/labels.txt', num_oov_indices=0)\n",
    "\n",
    "# Check the list of labels\n",
    "topic_lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 10000\n",
      "sample text: Dengue fever cases in Laos rise to 4256\n",
      "sample text (preprocessed): [4040 1979   30    2 9339  282    3    1    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Title length and vocabulary size used by the team for the prototype\n",
    "MAX_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "title_preprocessor.load_assets(vocab)\n",
    "\n",
    "# Check the vocabulary size\n",
    "print(f'vocabulary size: {title_preprocessor.vocabulary_size()}')\n",
    "\n",
    "# Get a sample title\n",
    "sample_title = train_df['title'][10]\n",
    "\n",
    "# Sample title in string format\n",
    "print(f\"sample text: {sample_title}\")\n",
    "\n",
    "# Sample title represented as an integer sequence\n",
    "print(f\"sample text (preprocessed): {title_preprocessor(sample_title)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763/763 [==============================] - 2s 2ms/step - loss: 1.0202 - sparse_categorical_accuracy: 0.7778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.020175576210022, 0.7778187394142151]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the test dataframe to a tf dataset\n",
    "test_ds = isbn_utils.df_to_tfdata(test_df, topic_lookup, title_preprocessor)\n",
    "\n",
    "# Get the metrics\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Load the train and test sets\n",
    "train_df = pd.read_csv(f'{data}/train_data.csv')\n",
    "test_df = pd.read_csv(f'{data}/test_data.csv')\n",
    "\n",
    "# Combine the two datasets. Set ignore_index to False.\n",
    "combined_df = pd.concat([train_df,test_df], ignore_index=True)\n",
    "\n",
    "# Split the combined dataset to 60% train, 20% dev, and 20% test set. Produce a balanced split along the `topic` column.\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['topic'])\n",
    "train_df, dev_df = train_test_split(train_df, test_size=0.25, stratify=train_df['topic'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './S2'\n",
    "\n",
    "# Set the subdirectories that will contain the experiment files\n",
    "data, model, vocab = isbn_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Save the datasets\n",
    "isbn_utils.save_data(train_df, data, 'train_data.csv')\n",
    "isbn_utils.save_data(dev_df, data, 'dev_data.csv')\n",
    "isbn_utils.save_data(test_df, data, 'test_data.csv')\n",
    "\n",
    "# Save the labels\n",
    "isbn_utils.save_labels(topic_lookup, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Working folder for the experiment\n",
    "BASE_DIR = './S1'\n",
    "\n",
    "# Get the subdirectories that contain the experiment files\n",
    "_, model_d, _ = isbn_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_d)\n",
    "\n",
    "# Working folder for the experiment\n",
    "BASE_DIR = './S2'\n",
    "\n",
    "# Title length and vocabulary size \n",
    "MAX_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Get the subdirectories that contain the experiment files\n",
    "data_d, model_d, vocab_d = isbn_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the train and test sets\n",
    "train_df = pd.read_csv(f'{data_d}/train_data.csv')\n",
    "dev_df = pd.read_csv(f'{data_d}/dev_data.csv')\n",
    "test_df = pd.read_csv(f'{data_d}/test_data.csv')\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Create a lookup list for the labels\n",
    "topic_lookup = tf.keras.layers.StringLookup(vocabulary=f'{vocab_d}/labels.txt', num_oov_indices=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the titles from the new training set\n",
    "train_inputs = train_df['title']\n",
    "\n",
    "# Generate a new vocabulary\n",
    "title_preprocessor.adapt(train_inputs)\n",
    "\n",
    "# Save the new vocabulary\n",
    "isbn_utils.save_vocab(title_preprocessor, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040/2040 [==============================] - 10s 5ms/step - loss: 0.8601 - sparse_categorical_accuracy: 0.6900 - val_loss: 0.5708 - val_sparse_categorical_accuracy: 0.8036\n",
      "Epoch 2/5\n",
      "2040/2040 [==============================] - 9s 5ms/step - loss: 0.4460 - sparse_categorical_accuracy: 0.8473 - val_loss: 0.5369 - val_sparse_categorical_accuracy: 0.8182\n",
      "Epoch 3/5\n",
      "2040/2040 [==============================] - 10s 5ms/step - loss: 0.3561 - sparse_categorical_accuracy: 0.8775 - val_loss: 0.5596 - val_sparse_categorical_accuracy: 0.8150\n",
      "Epoch 4/5\n",
      "2040/2040 [==============================] - 10s 5ms/step - loss: 0.2941 - sparse_categorical_accuracy: 0.9008 - val_loss: 0.5994 - val_sparse_categorical_accuracy: 0.8121\n",
      "Epoch 5/5\n",
      "2040/2040 [==============================] - 10s 5ms/step - loss: 0.2411 - sparse_categorical_accuracy: 0.9209 - val_loss: 0.6576 - val_sparse_categorical_accuracy: 0.8069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1cbbb37e520>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Convert the string datasets to Tensorflow datasets\n",
    "train_ds = isbn_utils.df_to_tfdata(train_df, topic_lookup, title_preprocessor, shuffle=True)\n",
    "dev_ds = isbn_utils.df_to_tfdata(dev_df, topic_lookup, title_preprocessor)\n",
    "test_ds = isbn_utils.df_to_tfdata(test_df, topic_lookup, title_preprocessor)\n",
    "\n",
    "# Reset the model weights\n",
    "model = isbn_utils.model_reset_weights(model)\n",
    "\n",
    "# Train the model. Use the dev set to check if your model is overfitting.\n",
    "model.fit(train_ds, epochs=NUM_EPOCHS, validation_data=dev_ds, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/680 [..............................] - ETA: 12s - loss: 0.5626 - sparse_categorical_accuracy: 0.8438"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 2ms/step - loss: 0.6459 - sparse_categorical_accuracy: 0.8097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6458941698074341, 0.8097448945045471]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the loss and metrics\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./S2/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./S2/model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(model_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY PER TOPIC:\n",
      "\n",
      "ENTERTAINMENT: 81.80 \n",
      "HEALTH: 81.00 \n",
      "TECHNOLOGY: 87.80 \n",
      "WORLD: 62.23 \n",
      "BUSINESS: 100.00 \n",
      "SPORTS: 89.13 \n",
      "NATION: 63.77 \n",
      "SCIENCE: 77.09 \n"
     ]
    }
   ],
   "source": [
    "# Get the list of topics\n",
    "topics = topic_lookup.get_vocabulary()\n",
    "\n",
    "# Evaluate the model's performance for each topic\n",
    "isbn_utils.print_metric_per_topic(dev_df, topics, topic_lookup, title_preprocessor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>domain</th>\n",
       "      <th>published_date</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>https://www.thetimes.co.uk/article/sunaks-eat-out-to-help-out-scheme-boosts-ailing-hospitality-sector-9lwf5gkl5</td>\n",
       "      <td>thetimes.co.uk</td>\n",
       "      <td>2020-08-16 23:01:00</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>https://www.reuters.com/article/us-amp-results/australias-amp-outlines-400-million-plan-for-special-dividend-and-buyback-shares-soar-idUSKCN2590B1</td>\n",
       "      <td>reuters.com</td>\n",
       "      <td>2020-08-13 03:25:00</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>https://www.globenewswire.com/news-release/2020/08/05/2073493/0/en/Surface-Oncology-to-Present-at-the-2020-Wedbush-PacGrow-Healthcare-Virtual-Conference.html</td>\n",
       "      <td>globenewswire.com</td>\n",
       "      <td>2020-08-05 15:59:00</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>https://www.cbc.ca/news/canada/calgary/calgary-flames-50-50-1.5682787</td>\n",
       "      <td>cbc.ca</td>\n",
       "      <td>2020-08-12 00:10:00</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>https://www.cnbc.com/2020/08/17/hedge-fund-manager-dan-loeb-made-a-number-of-moves-last-quarter-here-are-the-highlights.html</td>\n",
       "      <td>cnbc.com</td>\n",
       "      <td>2020-08-17 15:22:00</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65214</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>https://www.cnn.com/2020/08/17/investing/barrick-gold-warren-buffett-trnd/index.html</td>\n",
       "      <td>cnn.com</td>\n",
       "      <td>2020-08-17 15:47:00</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65221</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>https://finance.yahoo.com/news/might-not-great-idea-buy-083145646.html</td>\n",
       "      <td>finance.yahoo.com</td>\n",
       "      <td>2020-08-09 08:31:00</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65228</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>https://www.bloomberg.com/tosv2.html?vid=&amp;uuid=f49dd880-dedf-11ea-b3c0-cd853cc2abc6&amp;url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wOC0xNS9waGlsaXBwaW5lLXNlYy1wcm9iZXMtY29tcGxhaW50LWFyZWl0LXRyYWRlLW9yZGVycy11bmV4ZWN1dGVk</td>\n",
       "      <td>bloomberg.com</td>\n",
       "      <td>2020-08-15 04:28:00</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65235</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>https://www.theguardian.com/business/2020/aug/11/coronavirus-730000-workers-fall-from-uk-payrolls-between-march-and-july</td>\n",
       "      <td>theguardian.com</td>\n",
       "      <td>2020-08-11 08:40:00</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65242</th>\n",
       "      <td>Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story</td>\n",
       "      <td>https://www.nytimes.com/reuters/2020/08/13/world/europe/13reuters-health-coronavirus-india-j-j.html</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>2020-08-13 12:03:00</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          title  \\\n",
       "28     Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "65     Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "67     Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "77     Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "82     Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "...                                                                         ...   \n",
       "65214  Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "65221  Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "65228  Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "65235  Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "65242  Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story   \n",
       "\n",
       "                                                                                                                                                                                                               link  \\\n",
       "28                                                                                                  https://www.thetimes.co.uk/article/sunaks-eat-out-to-help-out-scheme-boosts-ailing-hospitality-sector-9lwf5gkl5   \n",
       "65                                                               https://www.reuters.com/article/us-amp-results/australias-amp-outlines-400-million-plan-for-special-dividend-and-buyback-shares-soar-idUSKCN2590B1   \n",
       "67                                                    https://www.globenewswire.com/news-release/2020/08/05/2073493/0/en/Surface-Oncology-to-Present-at-the-2020-Wedbush-PacGrow-Healthcare-Virtual-Conference.html   \n",
       "77                                                                                                                                            https://www.cbc.ca/news/canada/calgary/calgary-flames-50-50-1.5682787   \n",
       "82                                                                                     https://www.cnbc.com/2020/08/17/hedge-fund-manager-dan-loeb-made-a-number-of-moves-last-quarter-here-are-the-highlights.html   \n",
       "...                                                                                                                                                                                                             ...   \n",
       "65214                                                                                                                          https://www.cnn.com/2020/08/17/investing/barrick-gold-warren-buffett-trnd/index.html   \n",
       "65221                                                                                                                                        https://finance.yahoo.com/news/might-not-great-idea-buy-083145646.html   \n",
       "65228  https://www.bloomberg.com/tosv2.html?vid=&uuid=f49dd880-dedf-11ea-b3c0-cd853cc2abc6&url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wOC0xNS9waGlsaXBwaW5lLXNlYy1wcm9iZXMtY29tcGxhaW50LWFyZWl0LXRyYWRlLW9yZGVycy11bmV4ZWN1dGVk   \n",
       "65235                                                                                      https://www.theguardian.com/business/2020/aug/11/coronavirus-730000-workers-fall-from-uk-payrolls-between-march-and-july   \n",
       "65242                                                                                                           https://www.nytimes.com/reuters/2020/08/13/world/europe/13reuters-health-coronavirus-india-j-j.html   \n",
       "\n",
       "                  domain       published_date     topic  \n",
       "28        thetimes.co.uk  2020-08-16 23:01:00  BUSINESS  \n",
       "65           reuters.com  2020-08-13 03:25:00  BUSINESS  \n",
       "67     globenewswire.com  2020-08-05 15:59:00  BUSINESS  \n",
       "77                cbc.ca  2020-08-12 00:10:00  BUSINESS  \n",
       "82              cnbc.com  2020-08-17 15:22:00  BUSINESS  \n",
       "...                  ...                  ...       ...  \n",
       "65214            cnn.com  2020-08-17 15:47:00  BUSINESS  \n",
       "65221  finance.yahoo.com  2020-08-09 08:31:00  BUSINESS  \n",
       "65228      bloomberg.com  2020-08-15 04:28:00  BUSINESS  \n",
       "65235    theguardian.com  2020-08-11 08:40:00  BUSINESS  \n",
       "65242        nytimes.com  2020-08-13 12:03:00  BUSINESS  \n",
       "\n",
       "[9000 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the training dataframe's business articles\n",
    "train_df[train_df.topic=='BUSINESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Ontario rejects TDSB's reopening plan for elementary and secondary students\n",
      "\n",
      "label: SPORTS\n",
      "prediction: NATION\n",
      "title: Scott Pendlebury issues Twitter response over article claiming he was given permission to call Heretier Lumumba ‘chimp’\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: COVID-19 alters Indian Independence Day celebrations\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Storm Team 11: Hit or miss storms return Friday afternoon\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Ecological disaster in kite paradise!\n",
      "\n",
      "label: HEALTH\n",
      "prediction: NATION\n",
      "title: Melbourne’s quarantine hotels still earning as rooms lie empty\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Finally, Ogun Schools Resume Amid COVID-19 Protocols\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: WRDSB releases back-to-school plan, parents must fill out attendance form\n",
      "\n",
      "label: TECHNOLOGY\n",
      "prediction: NATION\n",
      "title: Teenage thug who stamped on man's head in unprovoked attack is jailed for more than seven years\n",
      "\n",
      "label: SPORTS\n",
      "prediction: NATION\n",
      "title: New York Jets Flight Connections 8/3/20\n",
      "\n",
      "label: HEALTH\n",
      "prediction: NATION\n",
      "title: Melton MP addresses local lockdown rumours\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: U.S. rollback of campus sexual assault rules takes effect Friday after judge's ruling\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Police treating body found in Newry as suspicious\n",
      "\n",
      "label: TECHNOLOGY\n",
      "prediction: NATION\n",
      "title: Dollar nurses losses ahead of non-farm payrolls data\n",
      "\n",
      "label: SPORTS\n",
      "prediction: NATION\n",
      "title: Coronavirus Updates: Boxer L Sarita Devi, husband Thoiba Singh test COVID-19 positive\n",
      "\n",
      "label: SPORTS\n",
      "prediction: NATION\n",
      "title: Wawrinka Headlines Loaded Prague Challenger Draw\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Lockdown cigarette wars: Govt admits smokers ‘less likely to be infected’ with coronavirus\n",
      "\n",
      "label: SPORTS\n",
      "prediction: NATION\n",
      "title: Anonymous Golden Knights donate pizzas to homeless shelter in Edmonton\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: President Kenyatta Says Kenya Ready Again For Visitors\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Fire crews out as lightning strikes 15 homes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get examples in the dev set that predicted `NATION` but the ground truth label is different\n",
    "isbn_utils.get_errors(model, dev_df, title_preprocessor, topic_lookup, 'NATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment folder\n",
    "BASE_DIR = './S3'\n",
    "\n",
    "# Set the subdirectories that will contain the experiment files\n",
    "data_d, model_d, vocab_d = isbn_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the backup CSV\n",
    "combined_df = pd.read_csv(f'./.backup.csv')\n",
    "\n",
    "# Generate train, dev, and test sets as you did before.\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['topic'])\n",
    "train_df, dev_df = train_test_split(train_df, test_size=0.25, stratify=train_df['topic'])\n",
    "\n",
    "# Save the datasets under the E3 folder\n",
    "isbn_utils.save_data(train_df, data_d, 'train_data.csv')\n",
    "isbn_utils.save_data(dev_df, data_d, 'dev_data.csv')\n",
    "isbn_utils.save_data(test_df, data_d, 'test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a new vocabulary based on the new training set\n",
    "train_inputs = train_df['title']\n",
    "title_preprocessor.adapt(train_inputs)\n",
    "\n",
    "# Save the new vocabulary and labels\n",
    "isbn_utils.save_vocab(title_preprocessor, vocab_d)\n",
    "isbn_utils.save_labels(topic_lookup, vocab_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  23/2040 [..............................] - ETA: 9s - loss: 2.0718 - sparse_categorical_accuracy: 0.1508"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040/2040 [==============================] - 10s 5ms/step - loss: 1.3485 - sparse_categorical_accuracy: 0.5057 - val_loss: 0.8473 - val_sparse_categorical_accuracy: 0.7073\n",
      "Epoch 2/5\n",
      "2040/2040 [==============================] - 10s 5ms/step - loss: 0.6572 - sparse_categorical_accuracy: 0.7768 - val_loss: 0.7175 - val_sparse_categorical_accuracy: 0.7622\n",
      "Epoch 3/5\n",
      "2040/2040 [==============================] - 9s 4ms/step - loss: 0.5075 - sparse_categorical_accuracy: 0.8296 - val_loss: 0.7351 - val_sparse_categorical_accuracy: 0.7610\n",
      "Epoch 4/5\n",
      "2040/2040 [==============================] - 9s 4ms/step - loss: 0.4301 - sparse_categorical_accuracy: 0.8567 - val_loss: 0.7751 - val_sparse_categorical_accuracy: 0.7572\n",
      "Epoch 5/5\n",
      "2040/2040 [==============================] - 9s 4ms/step - loss: 0.3694 - sparse_categorical_accuracy: 0.8773 - val_loss: 0.8333 - val_sparse_categorical_accuracy: 0.7546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1cbbb37e760>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Convert the dataframes to numeric features. Remember to shuffle the training set.\n",
    "train_ds = isbn_utils.df_to_tfdata(train_df, topic_lookup, title_preprocessor, shuffle=True)\n",
    "dev_ds = isbn_utils.df_to_tfdata(dev_df, topic_lookup, title_preprocessor)\n",
    "test_ds = isbn_utils.df_to_tfdata(test_df, topic_lookup, title_preprocessor)\n",
    "\n",
    "# Reset the model weights\n",
    "model = isbn_utils.model_reset_weights(model)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ds, epochs=NUM_EPOCHS, validation_data=dev_ds, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 2ms/step - loss: 0.8367 - sparse_categorical_accuracy: 0.7546\n",
      "INFO:tensorflow:Assets written to: ./S3/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./S3/model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set and write the results on the experiment tracker\n",
    "model.evaluate(test_ds)\n",
    "\n",
    "# Save the model to model_dir\n",
    "model.save(model_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY PER TOPIC:\n",
      "\n",
      "ENTERTAINMENT: 82.80 \n",
      "HEALTH: 78.97 \n",
      "TECHNOLOGY: 85.10 \n",
      "WORLD: 62.10 \n",
      "BUSINESS: 75.27 \n",
      "SPORTS: 86.27 \n",
      "NATION: 56.87 \n",
      "SCIENCE: 78.94 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "isbn_utils.print_metric_per_topic(dev_df, topics, topic_lookup, title_preprocessor, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Broke and helpless: Philippines COVID-19 lockdown maroons dozens at airport\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Faith leaders across the world unite to condemn China's brutal repression of Uighur Muslim minority\n",
      "\n",
      "label: BUSINESS\n",
      "prediction: NATION\n",
      "title: Daimler settlement over diesel emissions\n",
      "\n",
      "label: SPORTS\n",
      "prediction: NATION\n",
      "title: BYU, Utah State combined for 103 positive cases of COVID-19, per NY Times survey\n",
      "\n",
      "label: BUSINESS\n",
      "prediction: NATION\n",
      "title: Amazon remove shoes from sale after David Lammy MP highlights n-word description\n",
      "\n",
      "label: TECHNOLOGY\n",
      "prediction: NATION\n",
      "title: I’m Still Not Over the Bob-omb Thing\n",
      "\n",
      "label: ENTERTAINMENT\n",
      "prediction: NATION\n",
      "title: BBC radio host quits over use of N-word in news report\n",
      "\n",
      "label: HEALTH\n",
      "prediction: NATION\n",
      "title: Coronavirus Australia: Crucial COVID-19 deadline looms\n",
      "\n",
      "label: HEALTH\n",
      "prediction: NATION\n",
      "title: Students may be fast-tracked to fill WA doctor shortage as warnings sound over threat of second wave\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: India landslide: Dozens feared dead after flooding in Kerala\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: The Monitor :: Local Firms Win Big At Virtual SADC Challenge\n",
      "\n",
      "label: HEALTH\n",
      "prediction: NATION\n",
      "title: Closing arguments being heard in court challenge of Newfoundland and Labrador’s COVID-19 travel ban\n",
      "\n",
      "label: HEALTH\n",
      "prediction: NATION\n",
      "title: Listeners of Adelaide radio station SAFM scammed out of $12,000 by Canadian conwoman in 2008\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Indonesian docs on the lookout for producers, compelling storytelling style\n",
      "\n",
      "label: HEALTH\n",
      "prediction: NATION\n",
      "title: 'We're not out of the woods at all,' Ontario's top doc warns amid Muskoka outbreak\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Army to probe encounter of 3 ‘militants’ after families say they were labourers\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Shooting of Vietnamese fisherman: Firearm rules followed, says MMEA\n",
      "\n",
      "label: ENTERTAINMENT\n",
      "prediction: NATION\n",
      "title: ‘It’s stalking our neighbourhood’: Horse killed by cougar leaves residents near Qualicum Beach on edge\n",
      "\n",
      "label: SPORTS\n",
      "prediction: NATION\n",
      "title: Cash vs Welborn: Shannon Courtenay promises 'bite, spite and hurt' in big-pressure Fight Camp atmosphere\n",
      "\n",
      "label: WORLD\n",
      "prediction: NATION\n",
      "title: Trump bashes deceased civil rights leader for skipping his inauguration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get examples in the dev set that predicted `NATION` but the ground truth label is different\n",
    "isbn_utils.get_errors(model, dev_df, title_preprocessor, topic_lookup, 'NATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 2ms/step - loss: 0.8333 - sparse_top_k_categorical_accuracy: 0.8868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8332531452178955, 0.8868306279182434]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the top-K accuracy to 2\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=2)]             \n",
    "             )\n",
    "\n",
    "# Check the accuracy\n",
    "model.evaluate(dev_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY PER TOPIC:\n",
      "\n",
      "ENTERTAINMENT: 90.87 \n",
      "HEALTH: 90.70 \n",
      "TECHNOLOGY: 93.17 \n",
      "WORLD: 85.87 \n",
      "BUSINESS: 85.83 \n",
      "SPORTS: 90.67 \n",
      "NATION: 84.00 \n",
      "SCIENCE: 87.42 \n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy per topic\n",
    "isbn_utils.print_metric_per_topic(dev_df, topics, topic_lookup, title_preprocessor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 20, 24)            240000    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20, 24)            600       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 480)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 3848      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244448 (954.88 KB)\n",
      "Trainable params: 244448 (954.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "EMBEDDING_DIM = 24\n",
    "DENSE_DIM = 24\n",
    "topic_size = topic_lookup.vocabulary_size()\n",
    "\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "    tf.keras.layers.Dense(DENSE_DIM, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(topic_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVING_DIR: d:\\Mirela\\Visual Studio Code\\Work/serving\n",
      "os.environ[\"SERVING_DIR\"]: d:\\Mirela\\Visual Studio Code\\Work/serving\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SERVING_DIR = f'{os.getcwd()}/serving'\n",
    "os.environ[\"SERVING_DIR\"] = SERVING_DIR\n",
    "print(f'SERVING_DIR: {SERVING_DIR}')\n",
    "print(f'os.environ[\"SERVING_DIR\"]: {os.environ[\"SERVING_DIR\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Mirela\\\\Visual Studio Code\\\\Work/serving/1'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.makedirs(f'{SERVING_DIR}/1', exist_ok=True)\n",
    "shutil.copytree('./S2/model/', f'{SERVING_DIR}/1', dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "docker: error during connect: Head \"http://%2F%2F.%2Fpipe%2FdockerDesktopLinuxEngine/_ping\": open //./pipe/dockerDesktopLinuxEngine: The system cannot find the file specified.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "!docker run -t --rm -d -p 8501:8501 -v \"d:\\Mirela\\Visual Studio Code\\Work\\serving:/models/newsapp_model\" -e MODEL_NAME=newsapp_model tensorflow/serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=8501): Max retries exceeded with url: /v1/models/newsapp_model:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001CBC5B08B80>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\urllib3\\connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 495\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\urllib3\\connection.py:441\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\http\\client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1251\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\http\\client.py:1011\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1011\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1014\u001b[0m \n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\http\\client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\urllib3\\connection.py:279\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\urllib3\\connection.py:214\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000001CBC5B08B80>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=8501): Max retries exceeded with url: /v1/models/newsapp_model:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001CBC5B08B80>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m json_response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:8501/v1/models/newsapp_model:predict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Get the predictions\u001b[39;00m\n\u001b[0;32m     35\u001b[0m predictions \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_response\u001b[38;5;241m.\u001b[39mtext)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Mirela\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8501): Max retries exceeded with url: /v1/models/newsapp_model:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001CBC5B08B80>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))"
     ]
    }
   ],
   "source": [
    "# Used by the team on their prototype\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "MAX_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "title_preprocessor.load_assets('./S2/vocab')\n",
    "\n",
    "# Sample input\n",
    "sample_input = 'Sample title'\n",
    "\n",
    "# Preprocess the string\n",
    "sample_input_ds = title_preprocessor(sample_input)\n",
    "\n",
    "# Add a batch dimension\n",
    "sample_input_ds = tf.expand_dims(sample_input_ds, axis=0)\n",
    "\n",
    "# Compose the data\n",
    "data = json.dumps({\"instances\": sample_input_ds.numpy().tolist()})\n",
    "\n",
    "# Define the headers\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "\n",
    "# Send the request\n",
    "json_response = requests.post('http://localhost:8501/v1/models/newsapp_model:predict', data=data, headers=headers)\n",
    "\n",
    "# Get the predictions\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.9082040e-02, 1.4640763e-01, 3.8529795e-02, 5.0651237e-02,\n",
       "        4.4071399e-05, 3.5467157e-01, 8.7184235e-02, 2.3342945e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working folder for the experiment\n",
    "BASE_DIR = './S2'\n",
    "\n",
    "# get the subdirectories that contain the experiment files\n",
    "data_dir, model_dir, vocab_dir = isbn_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "title_preprocessor.load_assets(vocab_dir)\n",
    "\n",
    "# Attach the preprocessing layer to the trained model\n",
    "model_with_preprocessor = tf.keras.Sequential([\n",
    "    title_preprocessor,\n",
    "    model\n",
    "])\n",
    "\n",
    "# String input\n",
    "sample_input = \"Sample Title\"\n",
    "\n",
    "# Feed the string input directly to the model\n",
    "model_with_preprocessor.predict([sample_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SERVING_DIR = f'{os.getcwd()}/serving'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\Mirela\\Visual Studio Code\\Work/serving/2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\Mirela\\Visual Studio Code\\Work/serving/2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'd:\\Mirela\\Visual Studio Code\\Work/serving/2'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  Args:\n",
      "    args_0: string Tensor, shape=(None,)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(None, 8)\n"
     ]
    }
   ],
   "source": [
    "model_with_preprocessor.export(f'{SERVING_DIR}/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0890820399, 0.146407634, 0.0385297947, 0.0506512374, 4.40714e-05, 0.354671568, 0.0871842355, 0.233429447]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "data = json.dumps({\"instances\": [\"sample title\"]})\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/newsapp_model:predict', data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "con = sqlite3.connect(\"news_articles.db\")\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Waikato Times: Hamilton and Waikato news')\n",
      "(2, 'Southland Times: Southland /Invercargill news')\n",
      "(3, 'Sports: Rugby, cricket, football, All Blacks')\n",
      "(4, 'Real-Time News from PennLive')\n"
     ]
    }
   ],
   "source": [
    "# Preview some entries\n",
    "for row in cur.execute(\"SELECT id,title FROM news_articles WHERE id < 5\"):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection import isbn_utils\n",
    "import tensorflow as tf\n",
    "VOCAB_SIZE=10000\n",
    "MAX_LENGTH=20\n",
    "\n",
    "# working folder for the experiment\n",
    "BASE_DIR = './S1'\n",
    "\n",
    "# get the subdirectories that contain the experiment files\n",
    "_, model_dir, vocab_dir = isbn_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "title_preprocessor.load_assets(vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Initialize list containing the number of `1`'s per title\n",
    "unk_counts = []\n",
    "\n",
    "# Iterate over the titles in the database\n",
    "for row in cur.execute(\"SELECT title FROM news_articles\"):\n",
    "\n",
    "    # Convert the title to an integer sequence\n",
    "    sequence = title_preprocessor(row[0])\n",
    "\n",
    "    # Count the number of `1`\n",
    "    unk_count = np.count_nonzero(sequence == 1)\n",
    "\n",
    "    # Append to the list\n",
    "    unk_counts.append(unk_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ids\n",
    "ids = range(1, len(unk_counts)+1)\n",
    "\n",
    "# Plot the ids and unknown token counts\n",
    "plt.plot(ids, unk_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
